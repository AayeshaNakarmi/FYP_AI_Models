{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":160296,"sourceType":"datasetVersion","datasetId":72533},{"sourceId":6131005,"sourceType":"datasetVersion","datasetId":3515188},{"sourceId":4251,"sourceType":"modelInstanceVersion","modelInstanceId":3045,"modelId":575}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tune a Generative AI Model for Dialogue Summarization","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Here I am just sharing an interesting tutorial that was included as part of a course that I recently audited on Coursera.\n\nI thought that this tutorial did a great job of explaining parameter efficient fine-tuning methods (as well as providing examples for how to implement these methods).\n\n\nSource:\n - https://www.coursera.org/learn/generative-ai-with-llms\n - https://www.coursera.org/learn/generative-ai-with-llms/gradedLti/x0gc1/lab-2-fine-tune-a-generative-ai-model-for-dialogue-summarization\n\nhttps://creativecommons.org/licenses/by-sa/2.0/legalcode","metadata":{}},{"cell_type":"markdown","source":"In this notebook, you will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics.","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents","metadata":{}},{"cell_type":"markdown","source":"- [ 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM](#1)\n  - [ 1.1 - Set up Kernel and Required Dependencies](#1.1)\n  - [ 1.2 - Load Dataset and LLM](#1.2)\n  - [ 1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n- [ 2 - Perform Full Fine-Tuning](#2)\n  - [ 2.1 - Preprocess the Dialog-Summary Dataset](#2.1)\n  - [ 2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n  - [ 2.3 - Evaluate the Model Qualitatively (Human Evaluation)](#2.3)\n  - [ 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#2.4)\n- [ 3 - Perform Parameter Efficient Fine-Tuning (PEFT)](#3)\n  - [ 3.1 - Setup the PEFT/LoRA model for Fine-Tuning](#3.1)\n  - [ 3.2 - Train PEFT Adapter](#3.2)\n  - [ 3.3 - Evaluate the Model Qualitatively (Human Evaluation)](#3.3)\n  - [ 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#3.4)","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='1'></a>\n## 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM","metadata":{}},{"cell_type":"markdown","source":"<a name='1.1'></a>\n### 1.1 - Set up Kernel and Required Dependencies","metadata":{}},{"cell_type":"markdown","source":"Now install the required packages for the LLM and datasets.\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5JZ25vcmUgdGhlIHdhcm5pbmdzIGFuZCBlcnJvcnMsIGFsb25nIHdpdGggdGhlIG5vdGUgYWJvdXQgcmVzdGFydGluZyB0aGUga2VybmVsIGF0IHRoZSBlbmQuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>","metadata":{"tags":[]}},{"cell_type":"code","source":"%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.11.0 \\\n    evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    loralib==0.1.1 \\\n    peft==0.3.0 --quiet","metadata":{"tags":[],"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-11-28T07:01:33.318384Z","iopub.execute_input":"2024-11-28T07:01:33.319096Z","iopub.status.idle":"2024-11-28T07:04:09.390841Z","shell.execute_reply.started":"2024-11-28T07:01:33.319069Z","shell.execute_reply":"2024-11-28T07:04:09.389536Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.1.2)\nCollecting pip\n  Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.1.2\n    Uninstalling pip-23.1.2:\n      Successfully uninstalled pip-23.1.2\nSuccessfully installed pip-24.3.1\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"code","source":"!pip install awscli ","metadata":{"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:04:09.392807Z","iopub.execute_input":"2024-11-28T07:04:09.393119Z","iopub.status.idle":"2024-11-28T07:04:26.543815Z","shell.execute_reply.started":"2024-11-28T07:04:09.393076Z","shell.execute_reply":"2024-11-28T07:04:26.542971Z"}},"outputs":[{"name":"stdout","text":"Collecting awscli\n  Downloading awscli-1.36.12-py3-none-any.whl.metadata (11 kB)\nCollecting botocore==1.35.71 (from awscli)\n  Downloading botocore-1.35.71-py3-none-any.whl.metadata (5.7 kB)\nCollecting docutils<0.17,>=0.10 (from awscli)\n  Downloading docutils-0.16-py2.py3-none-any.whl.metadata (2.7 kB)\nCollecting s3transfer<0.11.0,>=0.10.0 (from awscli)\n  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: PyYAML<6.1,>=3.10 in /opt/conda/lib/python3.10/site-packages (from awscli) (6.0)\nRequirement already satisfied: colorama<0.4.7,>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from awscli) (0.4.6)\nCollecting rsa<4.8,>=3.1.2 (from awscli)\n  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore==1.35.71->awscli) (1.0.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore==1.35.71->awscli) (2.8.2)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore==1.35.71->awscli) (1.26.15)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.35.71->awscli) (1.16.0)\nDownloading awscli-1.36.12-py3-none-any.whl (4.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading botocore-1.35.71-py3-none-any.whl (13.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m143.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rsa-4.7.2-py3-none-any.whl (34 kB)\nDownloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\nInstalling collected packages: rsa, docutils, botocore, s3transfer, awscli\n  Attempting uninstall: rsa\n    Found existing installation: rsa 4.9\n    Uninstalling rsa-4.9:\n      Successfully uninstalled rsa-4.9\n  Attempting uninstall: docutils\n    Found existing installation: docutils 0.20.1\n    Uninstalling docutils-0.20.1:\n      Successfully uninstalled docutils-0.20.1\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.29.161\n    Uninstalling botocore-1.29.161:\n      Successfully uninstalled botocore-1.29.161\n  Attempting uninstall: s3transfer\n    Found existing installation: s3transfer 0.6.1\n    Uninstalling s3transfer-0.6.1:\n      Successfully uninstalled s3transfer-0.6.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.5.2 requires botocore<1.29.162,>=1.29.161, but you have botocore 1.35.71 which is incompatible.\nboto3 1.26.100 requires botocore<1.30.0,>=1.29.100, but you have botocore 1.35.71 which is incompatible.\nboto3 1.26.100 requires s3transfer<0.7.0,>=0.6.0, but you have s3transfer 0.10.4 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed awscli-1.36.12 botocore-1.35.71 docutils-0.16 rsa-4.7.2 s3transfer-0.10.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Import the necessary components. Some of them are new for this week, they will be discussed later in the notebook. ","metadata":{"tags":[]}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport evaluate\nimport pandas as pd\nimport numpy as np","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:04:26.544943Z","iopub.execute_input":"2024-11-28T07:04:26.545173Z","iopub.status.idle":"2024-11-28T07:04:45.604781Z","shell.execute_reply.started":"2024-11-28T07:04:26.545152Z","shell.execute_reply":"2024-11-28T07:04:45.604020Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"<a name='1.2'></a>\n### 1.2 - Load Dataset and LLM\n\nYou are going to continue experimenting with the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. ","metadata":{"tags":[]}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the SQuAD v2.0 dataset\ndataset = load_dataset(\"squad_v2\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:04:45.606673Z","iopub.execute_input":"2024-11-28T07:04:45.606919Z","iopub.status.idle":"2024-11-28T07:04:48.193050Z","shell.execute_reply.started":"2024-11-28T07:04:45.606897Z","shell.execute_reply":"2024-11-28T07:04:48.192153Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/8.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d83913101c3a4dce85e183b574b97a21"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset None/squad_v2 to /root/.cache/huggingface/datasets/parquet/squad_v2-ea29cd02ee439285/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0046469c5ba4f228d20a6c78c314349"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/16.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b376923d68df401191746b023a2ae9ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cd33d25ea104514a595af5212983bf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4afde086118f432c92e8bc01b8186106"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0afe0e1f9ed4290a97daf5f6e2df126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c08d4e6a08ee422583ea0178e5ee5267"}},"metadata":{}},{"name":"stdout","text":"Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/squad_v2-ea29cd02ee439285/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbb04b6521c54a9fa2e0c89752a3dfe0"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Preprocess the train and validation data\ndef preprocess_data(data):\n    articles = []\n    \n    # Iterate through each article in the dataset\n    for article in data:\n        context = article[\"context\"]  # context is a string for each article\n        question = article[\"question\"]  # question field\n        \n        # Store only context and question (as input-output pairs for QG)\n        articles.append({\n            'context': context,\n            'question': question\n        })\n    \n    return articles\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:04:48.194636Z","iopub.execute_input":"2024-11-28T07:04:48.195274Z","iopub.status.idle":"2024-11-28T07:04:48.200818Z","shell.execute_reply.started":"2024-11-28T07:04:48.195234Z","shell.execute_reply":"2024-11-28T07:04:48.199989Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(dataset['train'][0])  # Print the first element of the 'train' split to check the structure","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:04:48.201845Z","iopub.execute_input":"2024-11-28T07:04:48.202105Z","iopub.status.idle":"2024-11-28T07:04:48.252410Z","shell.execute_reply.started":"2024-11-28T07:04:48.202085Z","shell.execute_reply":"2024-11-28T07:04:48.251608Z"}},"outputs":[{"name":"stdout","text":"{'id': '56be85543aeaaa14008c9063', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train_data = preprocess_data(dataset['train'])\nval_data = preprocess_data(dataset['validation'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:04:48.253474Z","iopub.execute_input":"2024-11-28T07:04:48.253707Z","iopub.status.idle":"2024-11-28T07:04:56.055924Z","shell.execute_reply.started":"2024-11-28T07:04:48.253688Z","shell.execute_reply":"2024-11-28T07:04:56.055184Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Load the pre-trained [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer directly from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-base) of FLAN-T5. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by this model.","metadata":{"tags":[]}},{"cell_type":"code","source":"print(train_data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:04:56.056883Z","iopub.execute_input":"2024-11-28T07:04:56.057130Z","iopub.status.idle":"2024-11-28T07:04:56.061836Z","shell.execute_reply.started":"2024-11-28T07:04:56.057108Z","shell.execute_reply":"2024-11-28T07:04:56.061035Z"}},"outputs":[{"name":"stdout","text":"{'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?'}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Print the size of train_data and val_data\nprint(f\"Size of train_data: {len(train_data)}\")\nprint(f\"Size of val_data: {len(val_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:04:56.062882Z","iopub.execute_input":"2024-11-28T07:04:56.063123Z","iopub.status.idle":"2024-11-28T07:04:56.074545Z","shell.execute_reply.started":"2024-11-28T07:04:56.063102Z","shell.execute_reply":"2024-11-28T07:04:56.073782Z"}},"outputs":[{"name":"stdout","text":"Size of train_data: 130319\nSize of val_data: 11873\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# model_name='google/flan-t5-base'\nmodel_name='/kaggle/input/flan-t5/pytorch/base/4'\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:04:56.077362Z","iopub.execute_input":"2024-11-28T07:04:56.077601Z","iopub.status.idle":"2024-11-28T07:05:09.876399Z","shell.execute_reply.started":"2024-11-28T07:04:56.077582Z","shell.execute_reply":"2024-11-28T07:05:09.875427Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it. ","metadata":{"tags":[]}},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:05:09.877624Z","iopub.execute_input":"2024-11-28T07:05:09.877893Z","iopub.status.idle":"2024-11-28T07:05:09.885385Z","shell.execute_reply.started":"2024-11-28T07:05:09.877871Z","shell.execute_reply":"2024-11-28T07:05:09.884374Z"},"trusted":true},"outputs":[{"name":"stdout","text":"trainable model parameters: 247577856\nall model parameters: 247577856\npercentage of trainable model parameters: 100.00%\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(dataset.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:05:09.886727Z","iopub.execute_input":"2024-11-28T07:05:09.887103Z","iopub.status.idle":"2024-11-28T07:05:09.903623Z","shell.execute_reply.started":"2024-11-28T07:05:09.887070Z","shell.execute_reply":"2024-11-28T07:05:09.902633Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['train', 'validation'])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"<a name='1.3'></a>\n### 1.3 - Test the Model with Zero Shot Inferencing\n\nTest the model with the zero shot inferencing. You can see that the model struggles to summarize the dialogue compared to the baseline summary, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand.","metadata":{"tags":[]}},{"cell_type":"code","source":"index = 200\n\n# Use the 'validation' split (since 'test' does not exist)\ncontext = dataset['validation'][index]['context']\n\n# Update the prompt to focus on question generation based on the given context\nprompt = f\"\"\"\nGenerate a question based on the following context.\n\nContext:\n{context}\n\nQuestion:\n\"\"\"\n\n# Tokenize the prompt and feed it to the model for generation\ninputs = tokenizer(prompt, return_tensors='pt')\n\n# Generate the output (question) from the model\noutput = tokenizer.decode(\n    original_model.generate(\n        inputs[\"input_ids\"], \n        max_new_tokens=50,  # Adjust the number of tokens to control question length\n    )[0], \n    skip_special_tokens=True\n)\n\n# Print the output\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'MODEL GENERATED QUESTION:\\n{output}')\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:05:09.905045Z","iopub.execute_input":"2024-11-28T07:05:09.905966Z","iopub.status.idle":"2024-11-28T07:05:11.413739Z","shell.execute_reply.started":"2024-11-28T07:05:09.905933Z","shell.execute_reply":"2024-11-28T07:05:11.412768Z"},"trusted":true},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nGenerate a question based on the following context.\n\nContext:\nNormandy was the site of several important developments in the history of classical music in the 11th century. Fécamp Abbey and Saint-Evroul Abbey were centres of musical production and education. At Fécamp, under two Italian abbots, William of Volpiano and John of Ravenna, the system of denoting notes by letters was developed and taught. It is still the most common form of pitch representation in English- and German-speaking countries today. Also at Fécamp, the staff, around which neumes were oriented, was first developed and taught in the 11th century. Under the German abbot Isembard, La Trinité-du-Mont became a centre of musical composition.\n\nQuestion:\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATED QUESTION:\nWhat is the most common form of pitch representation in English- and German-speaking countries today?\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def tokenize_data(data, tokenizer, max_input_length=512, max_target_length=64):\n    tokenized_data = []\n\n    for article in data:\n        context = article[\"context\"]  # Context from the dataset\n        question = article[\"question\"]  # The question is the target\n\n        # Tokenize context (input text) and question (target text) separately\n        inputs = tokenizer(context, padding=\"max_length\", truncation=True, max_length=max_input_length, return_tensors=\"pt\")\n        targets = tokenizer(question, padding=\"max_length\", truncation=True, max_length=max_target_length, return_tensors=\"pt\")\n\n        tokenized_data.append({\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n            \"labels\": targets[\"input_ids\"].squeeze()\n        })\n    \n    return tokenized_data\n\n# Tokenize the preprocessed data (train and validation)\ntrain_tokenized = tokenize_data(train_data, tokenizer)\nval_tokenized = tokenize_data(val_data, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:05:11.414855Z","iopub.execute_input":"2024-11-28T07:05:11.415131Z","iopub.status.idle":"2024-11-28T07:07:45.764328Z","shell.execute_reply.started":"2024-11-28T07:05:11.415107Z","shell.execute_reply":"2024-11-28T07:07:45.763366Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass QuestionGenerationDataset(Dataset):\n    def __init__(self, tokenized_data):\n        self.data = tokenized_data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# Create datasets\ntrain_dataset = QuestionGenerationDataset(train_tokenized)\nval_dataset = QuestionGenerationDataset(val_tokenized)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:07:45.765593Z","iopub.execute_input":"2024-11-28T07:07:45.765867Z","iopub.status.idle":"2024-11-28T07:07:45.770897Z","shell.execute_reply.started":"2024-11-28T07:07:45.765844Z","shell.execute_reply":"2024-11-28T07:07:45.770021Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Create DataLoaders for training and validation\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8)\n\n# Example of iterating through the training batches\nfor batch in train_dataloader:\n    print(batch)\n    break  # Just to inspect the first batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:07:45.771988Z","iopub.execute_input":"2024-11-28T07:07:45.772253Z","iopub.status.idle":"2024-11-28T07:07:45.802017Z","shell.execute_reply.started":"2024-11-28T07:07:45.772221Z","shell.execute_reply":"2024-11-28T07:07:45.801246Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[  555, 11924,   127,  ...,     0,     0,     0],\n        [22738,   300,  3105,  ...,     0,     0,     0],\n        [ 9992,     7,    43,  ...,     0,     0,     0],\n        ...,\n        [ 8718,    13,     8,  ...,     0,     0,     0],\n        [ 2184,    47,  7513,  ...,     0,     0,     0],\n        [ 1013, 28226,    19,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 2645,  1204,     3,     9,  4970, 16524,    45,     8,     3, 21543,\n          8563,  1888,     6,     3,  3565,   112,   564,   271,     3,     7,\n         14528, 12153,   120,    30,     8, 16524,    58,     1,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [ 2645,  3028, 26252,    18,   279,   159,  7348,    38,   271,    44,\n          1020,    21,  2852,     3,     9,    96,    29,  4667,    32,    18,\n          5540,   121,    58,     1,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [  363,    19,    34,   718,   116,  1742,    13,     8,    72, 16346,\n             3,     7,   994,  5978,    28,   284,   119,    21,     8, 10901,\n            12,     3,  5058,    58,     1,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [   86,   125,   215,    47,     8,   564,  4263, 12873, 14560,  1528,\n          2730,   343,  5750,  7546,    58,     1,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [  363,  1041,   263,   380,    21,     8,   837,   993,    58,     1,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [13430,     9,  4109, 16884,     7,    45,   125,   686,    13,  4316,\n            58,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [ 2645,    47,  7513, 24391,    12,  9599, 13214,    28,  1317, 10634,\n            11, 11930,  8277,    58,     1,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [  363,    19,     8,  5294,    13, 13424,  5452,    16,  1013,  2740,\n             7,  1018,    58,     1,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0]])}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Print the size (length) of the datasets\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:07:45.802955Z","iopub.execute_input":"2024-11-28T07:07:45.803153Z","iopub.status.idle":"2024-11-28T07:07:45.807463Z","shell.execute_reply.started":"2024-11-28T07:07:45.803136Z","shell.execute_reply":"2024-11-28T07:07:45.806657Z"}},"outputs":[{"name":"stdout","text":"Number of training samples: 130319\nNumber of validation samples: 11873\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def tokenize_function(example):\n    start_prompt = 'Given the following context, create a detailed and contextually rich question.\\n\\n'\n    end_prompt = '\\n\\nQuestion: '\n    \n    # Concatenate the context with the start and end prompt\n    prompt = [start_prompt + context + end_prompt for context in example[\"context\"]]\n    \n    # Tokenize the input (context) and the target (question)\n    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    \n    # Use the question as the target (label)\n    example['labels'] = tokenizer(example[\"question\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    \n    return example\n\n# Map the tokenize function to all splits in the dataset (train, validation, test)\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Remove unnecessary columns\ntokenized_datasets = tokenized_datasets.remove_columns(['id', 'context', 'question', 'answers'])\n\n# Check the result (for train split)\nprint(tokenized_datasets['train'][0])  # Check the first element in the train split\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:07:45.808462Z","iopub.execute_input":"2024-11-28T07:07:45.808666Z","iopub.status.idle":"2024-11-28T07:08:56.177541Z","shell.execute_reply.started":"2024-11-28T07:07:45.808649Z","shell.execute_reply":"2024-11-28T07:08:56.176701Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3de558014e32489cb7e7dc7100a4d7e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e71894cbe37f4074bdac64af8da737f5"}},"metadata":{}},{"name":"stdout","text":"{'title': 'Beyoncé', 'input_ids': [9246, 8, 826, 2625, 6, 482, 3, 9, 3117, 11, 28131, 120, 2354, 822, 5, 493, 63, 106, 75, 154, 3156, 7, 693, 8900, 965, 18, 6936, 449, 41, 87, 115, 23, 2, 354, 2, 29, 7, 15, 2, 87, 36, 15, 18, 476, 4170, 18, 8735, 61, 41, 7473, 1600, 6464, 15465, 61, 19, 46, 797, 7634, 6, 3, 21101, 6, 1368, 8211, 11, 15676, 5, 12896, 11, 3279, 16, 8018, 6, 2514, 6, 255, 3032, 16, 796, 8782, 11, 10410, 2259, 7, 38, 3, 9, 861, 6, 11, 4659, 12, 10393, 16, 8, 1480, 5541, 7, 38, 991, 7634, 13, 391, 184, 279, 3202, 18, 10739, 19344, 63, 31, 7, 9364, 5, 19607, 26, 57, 160, 2353, 6, 9762, 15, 210, 8900, 965, 6, 8, 563, 1632, 80, 13, 8, 296, 31, 7, 200, 18, 17556, 3202, 1637, 13, 66, 97, 5, 2940, 7102, 144, 302, 1509, 8, 1576, 13, 493, 63, 106, 75, 154, 31, 7, 5695, 2306, 6, 2744, 1304, 11937, 16, 2129, 3, 31210, 6, 84, 2127, 160, 38, 3, 9, 6729, 2377, 4388, 6, 4964, 874, 26596, 6580, 11, 4510, 8, 3259, 1976, 5396, 910, 381, 18, 782, 712, 7, 96, 254, 7275, 63, 16, 2129, 121, 11, 96, 279, 9, 969, 7508, 1280, 11860, 10, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [366, 410, 493, 63, 14549, 456, 2852, 1012, 58, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"tokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# After tokenization, you can access the splits:\ntrain_dataset = tokenized_datasets['train']\nval_dataset = tokenized_datasets['validation']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:08:56.178505Z","iopub.execute_input":"2024-11-28T07:08:56.178768Z","iopub.status.idle":"2024-11-28T07:10:01.463374Z","shell.execute_reply.started":"2024-11-28T07:08:56.178746Z","shell.execute_reply":"2024-11-28T07:10:01.462691Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e679356363d43c89c651286f94a1074"}},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"To save some time in the lab, you will subsample the dataset:","metadata":{"tags":[]}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Print the size (length) of the datasets\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(val_dataset)}\")","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:10:01.464381Z","iopub.execute_input":"2024-11-28T07:10:01.464630Z","iopub.status.idle":"2024-11-28T07:10:01.469403Z","shell.execute_reply.started":"2024-11-28T07:10:01.464608Z","shell.execute_reply":"2024-11-28T07:10:01.468565Z"}},"outputs":[{"name":"stdout","text":"Number of training samples: 130319\nNumber of validation samples: 11873\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"The output dataset is ready for fine-tuning.","metadata":{}},{"cell_type":"markdown","source":"<a name='2.2'></a>\n### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n\nNow utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment.","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:10:01.470589Z","iopub.execute_input":"2024-11-28T07:10:01.471143Z","iopub.status.idle":"2024-11-28T07:10:03.105254Z","shell.execute_reply.started":"2024-11-28T07:10:01.471109Z","shell.execute_reply":"2024-11-28T07:10:03.104477Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"Start training process...\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5Zb3UgY2FuIHNhZmVseSBpZ25vcmUgdGhlIHdhcm5pbmcgbWVzc2FnZXMuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Training a fully fine-tuned version of the model would take a few hours on a GPU. To save time, download a checkpoint of the fully fine-tuned model to use in the rest of this notebook. This fully fine-tuned model will also be referred to as the **instruct model** in this lab.","metadata":{}},{"cell_type":"code","source":"# !aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:03.106256Z","iopub.execute_input":"2024-11-28T07:10:03.106494Z","iopub.status.idle":"2024-11-28T07:10:03.114577Z","shell.execute_reply.started":"2024-11-28T07:10:03.106473Z","shell.execute_reply":"2024-11-28T07:10:03.113946Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"The size of the downloaded instruct model is approximately 1GB.","metadata":{"tags":[]}},{"cell_type":"code","source":"# !ls -alh /kaggle/working/flan-dialogue-summary-checkpoint/pytorch_model.bin","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:03.115539Z","iopub.execute_input":"2024-11-28T07:10:03.115848Z","iopub.status.idle":"2024-11-28T07:10:03.125496Z","shell.execute_reply.started":"2024-11-28T07:10:03.115818Z","shell.execute_reply":"2024-11-28T07:10:03.124812Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"Create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='2.3'></a>\n### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)\n\nAs with many GenAI applications, a qualitative approach where you ask yourself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), you can see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model.","metadata":{}},{"cell_type":"markdown","source":"Evaluate the models computing ROUGE metrics. Notice the improvement in the results!","metadata":{"tags":[]}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=32, # Rank\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:10:03.126585Z","iopub.execute_input":"2024-11-28T07:10:03.127119Z","iopub.status.idle":"2024-11-28T07:10:03.149809Z","shell.execute_reply.started":"2024-11-28T07:10:03.127089Z","shell.execute_reply":"2024-11-28T07:10:03.149150Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Apply LoRA to your model\npeft_model = get_peft_model(original_model, lora_config)\n\n# Check the number of trainable parameters\ndef print_number_of_trainable_model_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Trainable parameters in PEFT model: {print_number_of_trainable_model_parameters(peft_model)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:10:03.150701Z","iopub.execute_input":"2024-11-28T07:10:03.150925Z","iopub.status.idle":"2024-11-28T07:10:03.739646Z","shell.execute_reply.started":"2024-11-28T07:10:03.150905Z","shell.execute_reply":"2024-11-28T07:10:03.738731Z"}},"outputs":[{"name":"stdout","text":"Trainable parameters in PEFT model: 3538944\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"Add LoRA adapter layers/parameters to the original LLM to be trained.","metadata":{"tags":[]}},{"cell_type":"code","source":"# Define the output directory for saving model checkpoints\noutput_dir = f'/kaggle/working/peft-flan-t5-squad-{str(int(time.time()))}'\n\n# Define TrainingArguments for PEFT fine-tuning\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=2e-5,  # Optimal learning rate for PEFT (higher than standard fine-tuning, but still small)\n    num_train_epochs=7,  # More epochs for better adaptation to the task\n    logging_steps=500,  # Log every 500 steps (adjust based on the dataset size)\n    evaluation_strategy=\"steps\",  # Evaluate every few steps\n    eval_steps=500,  # Evaluate every 500 steps (adjust for your dataset size)\n    save_steps=500,  # Save the model every 500 steps\n    per_device_train_batch_size=16,  # Batch size for training, adjust depending on available memory\n    per_device_eval_batch_size=16,  # Batch size for evaluation, adjust based on memory\n    warmup_steps=1000,  # Number of steps for learning rate warmup\n    weight_decay=0.01,  # Weight decay to prevent overfitting\n    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps to simulate larger batch size\n    fp16=True,  # Use mixed precision for faster training\n    report_to=None,  # Disable reporting to external services (you can enable if needed)\n    save_total_limit=3,  # Limit the number of saved checkpoints to avoid filling up disk space\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:10:03.741025Z","iopub.execute_input":"2024-11-28T07:10:03.741467Z","iopub.status.idle":"2024-11-28T07:10:03.827040Z","shell.execute_reply.started":"2024-11-28T07:10:03.741427Z","shell.execute_reply":"2024-11-28T07:10:03.826407Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Initialize the Trainer with model, training arguments, and datasets\npeft_trainer = Trainer(\n    model=peft_model,  # Make sure `original_model` is the FLAN-T5 model\n    args=peft_training_args,\n    train_dataset=train_dataset,  # Use your tokenized training dataset\n    eval_dataset=val_dataset,  # Use your tokenized validation dataset\n)\n\n# Start training\npeft_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T07:10:03.828049Z","iopub.execute_input":"2024-11-28T07:10:03.828585Z","iopub.status.idle":"2024-11-28T07:10:57.113039Z","shell.execute_reply.started":"2024-11-28T07:10:03.828551Z","shell.execute_reply":"2024-11-28T07:10:57.102974Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241128_071022-nbl96m8u</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/aayeshanakarmi-london-metropolitan-university/huggingface/runs/nbl96m8u' target=\"_blank\">swept-galaxy-5</a></strong> to <a href='https://wandb.ai/aayeshanakarmi-london-metropolitan-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/aayeshanakarmi-london-metropolitan-university/huggingface' target=\"_blank\">https://wandb.ai/aayeshanakarmi-london-metropolitan-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/aayeshanakarmi-london-metropolitan-university/huggingface/runs/nbl96m8u' target=\"_blank\">https://wandb.ai/aayeshanakarmi-london-metropolitan-university/huggingface/runs/nbl96m8u</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m10\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m)                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m# Start training\u001b[0m                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m10 peft_trainer.train()                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1633\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1630 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1631 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1632 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1633 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1634 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1635 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1636 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1902\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1899 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1900 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1901 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1902 \u001b[2m│   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1903 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1904 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1905 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2645\u001b[0m in \u001b[92mtraining_step\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2642 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m loss_mb.reduce_mean().detach().to(\u001b[96mself\u001b[0m.args.device)                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2643 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2644 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.compute_loss_context_manager():                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2645 \u001b[2m│   │   │   \u001b[0mloss = \u001b[96mself\u001b[0m.compute_loss(model, inputs)                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2646 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2647 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.n_gpu > \u001b[94m1\u001b[0m:                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2648 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = loss.mean()  \u001b[2m# mean() to average on multi-gpu parallel training\u001b[0m        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2677\u001b[0m in \u001b[92mcompute_loss\u001b[0m             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2674 \u001b[0m\u001b[2m│   │   │   \u001b[0mlabels = inputs.pop(\u001b[33m\"\u001b[0m\u001b[33mlabels\u001b[0m\u001b[33m\"\u001b[0m)                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2675 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2676 \u001b[0m\u001b[2m│   │   │   \u001b[0mlabels = \u001b[94mNone\u001b[0m                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2677 \u001b[2m│   │   \u001b[0moutputs = model(**inputs)                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2678 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Save past state if it exists\u001b[0m                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2679 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# TODO: this needs to be fixed and made cleaner later.\u001b[0m                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2680 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.past_index >= \u001b[94m0\u001b[0m:                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1194\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/\u001b[0m\u001b[1;33mdata_parallel.py\u001b[0m:\u001b[94m171\u001b[0m in \u001b[92mforward\u001b[0m        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(\u001b[96mself\u001b[0m.device_ids) == \u001b[94m1\u001b[0m:                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m169 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.module(*inputs[\u001b[94m0\u001b[0m], **kwargs[\u001b[94m0\u001b[0m])                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m│   │   │   \u001b[0mreplicas = \u001b[96mself\u001b[0m.replicate(\u001b[96mself\u001b[0m.module, \u001b[96mself\u001b[0m.device_ids[:\u001b[96mlen\u001b[0m(inputs)])          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m171 \u001b[2m│   │   │   \u001b[0moutputs = \u001b[96mself\u001b[0m.parallel_apply(replicas, inputs, kwargs)                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.gather(outputs, \u001b[96mself\u001b[0m.output_device)                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m173 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mreplicate\u001b[0m(\u001b[96mself\u001b[0m, module, device_ids):                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/\u001b[0m\u001b[1;33mdata_parallel.py\u001b[0m:\u001b[94m181\u001b[0m in \u001b[92mparallel_apply\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m178 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m scatter_kwargs(inputs, kwargs, device_ids, dim=\u001b[96mself\u001b[0m.dim)                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m179 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m180 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mparallel_apply\u001b[0m(\u001b[96mself\u001b[0m, replicas, inputs, kwargs):                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m181 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m parallel_apply(replicas, inputs, kwargs, \u001b[96mself\u001b[0m.device_ids[:\u001b[96mlen\u001b[0m(replicas)])   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m182 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m183 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mgather\u001b[0m(\u001b[96mself\u001b[0m, outputs, output_device):                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m184 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m gather(outputs, output_device, dim=\u001b[96mself\u001b[0m.dim)                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/\u001b[0m\u001b[1;33mparallel_apply.py\u001b[0m:\u001b[94m89\u001b[0m in \u001b[92mparallel_apply\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m86 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(\u001b[96mlen\u001b[0m(inputs)):                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m87 \u001b[0m\u001b[2m│   │   \u001b[0moutput = results[i]                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m88 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(output, ExceptionWrapper):                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m89 \u001b[2m│   │   │   \u001b[0moutput.reraise()                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m90 \u001b[0m\u001b[2m│   │   \u001b[0moutputs.append(output)                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m91 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m outputs                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m92 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/\u001b[0m\u001b[1;33m_utils.py\u001b[0m:\u001b[94m543\u001b[0m in \u001b[92mreraise\u001b[0m                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m540 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# If the exception takes multiple arguments, don't try to\u001b[0m                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m541 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# instantiate since we don't know how to\u001b[0m                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m542 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(msg) \u001b[94mfrom\u001b[0m \u001b[94mNone\u001b[0m                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m543 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m exception                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m544 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m545 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m546 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_get_available_device_type\u001b[0m():                                                          \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mOutOfMemoryError: \u001b[0mCaught OutOfMemoryError in replica \u001b[1;36m0\u001b[0m on device \u001b[1;36m0\u001b[0m.\nOriginal Traceback \u001b[1m(\u001b[0mmost recent call last\u001b[1m)\u001b[0m:\n  File \u001b[32m\"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\"\u001b[0m, line \u001b[1;36m64\u001b[0m, in _worker\n    output = \u001b[1;35mmodule\u001b[0m\u001b[1m(\u001b[0m*input, **kwargs\u001b[1m)\u001b[0m\n  File \u001b[32m\"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[1;36m1194\u001b[0m, in _call_impl\n    return \u001b[1;35mforward_call\u001b[0m\u001b[1m(\u001b[0m*input, **kwargs\u001b[1m)\u001b[0m\n  File \u001b[32m\"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\"\u001b[0m, line \u001b[1;36m869\u001b[0m, in forward\n    return \u001b[1;35mself.base_model\u001b[0m\u001b[1m(\u001b[0m\n  File \u001b[32m\"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[1;36m1194\u001b[0m, in _call_impl\n    return \u001b[1;35mforward_call\u001b[0m\u001b[1m(\u001b[0m*input, **kwargs\u001b[1m)\u001b[0m\n  File \u001b[32m\"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"\u001b[0m, line \u001b[1;36m1704\u001b[0m, in forward\n    decoder_outputs = \u001b[1;35mself.decoder\u001b[0m\u001b[1m(\u001b[0m\n  File \u001b[32m\"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[1;36m1194\u001b[0m, in _call_impl\n    return \u001b[1;35mforward_call\u001b[0m\u001b[1m(\u001b[0m*input, **kwargs\u001b[1m)\u001b[0m\n  File \u001b[32m\"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"\u001b[0m, line \u001b[1;36m1074\u001b[0m, in forward\n    layer_outputs = \u001b[1;35mlayer_module\u001b[0m\u001b[1m(\u001b[0m\n  File \u001b[32m\"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[1;36m1194\u001b[0m, in _call_impl\n    return \u001b[1;35mforward_call\u001b[0m\u001b[1m(\u001b[0m*input, **kwargs\u001b[1m)\u001b[0m\n  File \u001b[32m\"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"\u001b[0m, line \u001b[1;36m719\u001b[0m, in forward\n    cross_attention_outputs = self.layer\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m(\u001b[0m\n  File \u001b[32m\"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[1;36m1194\u001b[0m, in _call_impl\n    return \u001b[1;35mforward_call\u001b[0m\u001b[1m(\u001b[0m*input, **kwargs\u001b[1m)\u001b[0m\n  File \u001b[32m\"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"\u001b[0m, line \u001b[1;36m634\u001b[0m, in forward\n    attention_output = \u001b[1;35mself.EncDecAttention\u001b[0m\u001b[1m(\u001b[0m\n  File \u001b[32m\"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[1;36m1194\u001b[0m, in _call_impl\n    return \u001b[1;35mforward_call\u001b[0m\u001b[1m(\u001b[0m*input, **kwargs\u001b[1m)\u001b[0m\n  File \u001b[32m\"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"\u001b[0m, line \u001b[1;36m560\u001b[0m, in forward\n    attn_weights = \u001b[1;35mnn.functional.softmax\u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mscores.float\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mdim\u001b[0m=\u001b[1;36m-1\u001b[0m\u001b[1m)\u001b[0m\u001b[1;35m.type_as\u001b[0m\u001b[1m(\u001b[0m\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate \u001b[1;36m192.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m14.74\u001b[0m GiB total capacity; \n\u001b[1;36m14.34\u001b[0m GiB already allocated; \u001b[1;36m188.12\u001b[0m MiB free; \u001b[1;36m14.37\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> \nallocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and\nPYTORCH_CUDA_ALLOC_CONF\n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>)                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 # Start training</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>10 peft_trainer.train()                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1633</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1630 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1631 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1632 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1633 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1634 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1635 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1636 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1902</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1899 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> model.no_sync():                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1900 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1901 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1902 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1903 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1904 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1905 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2645</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2642 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss_mb.reduce_mean().detach().to(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.device)                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2643 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2644 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.compute_loss_context_manager():                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2645 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.compute_loss(model, inputs)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2646 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2647 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.n_gpu &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2648 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss = loss.mean()  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># mean() to average on multi-gpu parallel training</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2677</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">compute_loss</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2674 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>labels = inputs.pop(<span style=\"color: #808000; text-decoration-color: #808000\">\"labels\"</span>)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2675 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2676 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>labels = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2677 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = model(**inputs)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2678 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Save past state if it exists</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2679 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># TODO: this needs to be fixed and made cleaner later.</span>                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2680 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.past_index &gt;= <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">data_parallel.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">171</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device_ids) == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">169 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.module(*inputs[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>], **kwargs[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>])                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>replicas = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.replicate(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.module, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device_ids[:<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(inputs)])          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>171 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.parallel_apply(replicas, inputs, kwargs)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gather(outputs, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.output_device)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">173 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">replicate</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, module, device_ids):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">data_parallel.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">181</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">parallel_apply</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">178 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> scatter_kwargs(inputs, kwargs, device_ids, dim=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dim)                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">179 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">180 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">parallel_apply</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, replicas, inputs, kwargs):                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>181 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> parallel_apply(replicas, inputs, kwargs, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device_ids[:<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(replicas)])   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">182 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">183 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">gather</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, outputs, output_device):                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">184 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> gather(outputs, output_device, dim=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dim)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">parallel_apply.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">89</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">parallel_apply</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">86 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(inputs)):                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">87 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>output = results[i]                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">88 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(output, ExceptionWrapper):                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>89 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output.reraise()                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">90 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs.append(output)                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">91 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> outputs                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">92 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">543</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">reraise</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">540 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># If the exception takes multiple arguments, don't try to</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">541 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># instantiate since we don't know how to</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">542 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">RuntimeError</span>(msg) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>543 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> exception                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">544 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">545 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">546 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_get_available_device_type</span>():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>Caught OutOfMemoryError in replica <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> on device <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.\nOriginal Traceback <span style=\"font-weight: bold\">(</span>most recent call last<span style=\"font-weight: bold\">)</span>:\n  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, in _worker\n    output = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">module</span><span style=\"font-weight: bold\">(</span>*input, **kwargs<span style=\"font-weight: bold\">)</span>\n  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1194</span>, in _call_impl\n    return <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">forward_call</span><span style=\"font-weight: bold\">(</span>*input, **kwargs<span style=\"font-weight: bold\">)</span>\n  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">869</span>, in forward\n    return <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">self.base_model</span><span style=\"font-weight: bold\">(</span>\n  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1194</span>, in _call_impl\n    return <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">forward_call</span><span style=\"font-weight: bold\">(</span>*input, **kwargs<span style=\"font-weight: bold\">)</span>\n  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1704</span>, in forward\n    decoder_outputs = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">self.decoder</span><span style=\"font-weight: bold\">(</span>\n  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1194</span>, in _call_impl\n    return <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">forward_call</span><span style=\"font-weight: bold\">(</span>*input, **kwargs<span style=\"font-weight: bold\">)</span>\n  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1074</span>, in forward\n    layer_outputs = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">layer_module</span><span style=\"font-weight: bold\">(</span>\n  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1194</span>, in _call_impl\n    return <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">forward_call</span><span style=\"font-weight: bold\">(</span>*input, **kwargs<span style=\"font-weight: bold\">)</span>\n  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">719</span>, in forward\n    cross_attention_outputs = self.layer<span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">](</span>\n  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1194</span>, in _call_impl\n    return <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">forward_call</span><span style=\"font-weight: bold\">(</span>*input, **kwargs<span style=\"font-weight: bold\">)</span>\n  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">634</span>, in forward\n    attention_output = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">self.EncDecAttention</span><span style=\"font-weight: bold\">(</span>\n  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1194</span>, in _call_impl\n    return <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">forward_call</span><span style=\"font-weight: bold\">(</span>*input, **kwargs<span style=\"font-weight: bold\">)</span>\n  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">560</span>, in forward\n    attn_weights = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">nn.functional.softmax</span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">scores.float</span><span style=\"font-weight: bold\">()</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dim</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span><span style=\"font-weight: bold\">)</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.type_as</span><span style=\"font-weight: bold\">(</span>\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">192.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.74</span> GiB total capacity; \n<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.34</span> GiB already allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">188.12</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.37</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; \nallocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and\nPYTORCH_CUDA_ALLOC_CONF\n\n</pre>\n"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"<a name='3.2'></a>\n### 3.2 - Train PEFT Adapter\n\nDefine training arguments and create `Trainer` instance.","metadata":{"tags":[]}},{"cell_type":"code","source":"output_dir = f'/kaggle/working/peft-dialogue-summary-training-{str(int(time.time()))}'\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n    num_train_epochs=1,\n    logging_steps=1,\n    max_steps=1    \n)\n    \npeft_trainer = Trainer(\n    model=peft_model,\n    args=peft_training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:57.114060Z","iopub.status.idle":"2024-11-28T07:10:57.114416Z","shell.execute_reply.started":"2024-11-28T07:10:57.114268Z","shell.execute_reply":"2024-11-28T07:10:57.114284Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now everything is ready to train the PEFT adapter and save the model.\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>","metadata":{}},{"cell_type":"code","source":"peft_trainer.train()\n\npeft_model_path=\"/kaggle/working/peft-dialogue-summary-checkpoint-local\"\n\npeft_trainer.model.save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:57.115359Z","iopub.status.idle":"2024-11-28T07:10:57.115656Z","shell.execute_reply.started":"2024-11-28T07:10:57.115505Z","shell.execute_reply":"2024-11-28T07:10:57.115517Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"That training was performed on a subset of data. To load a fully trained PEFT model, read a checkpoint of a PEFT model from S3.","metadata":{"tags":[]}},{"cell_type":"code","source":"# !aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ /kaggle/working/peft-dialogue-summary-checkpoint-from-s3/ ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:57.117357Z","iopub.status.idle":"2024-11-28T07:10:57.117807Z","shell.execute_reply.started":"2024-11-28T07:10:57.117577Z","shell.execute_reply":"2024-11-28T07:10:57.117597Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check that the size of this model is much less than the original LLM:","metadata":{"tags":[]}},{"cell_type":"code","source":"# !ls -al /kaggle/working/peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:57.119063Z","iopub.status.idle":"2024-11-28T07:10:57.119533Z","shell.execute_reply.started":"2024-11-28T07:10:57.119300Z","shell.execute_reply":"2024-11-28T07:10:57.119322Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Prepare this model by adding an adapter to the original FLAN-T5 model. You are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set `is_trainable=True`.","metadata":{"tags":[]}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\npeft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/input/flan-t5/pytorch/base/4\", torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/flan-t5/pytorch/base/4\")\n\npeft_model = PeftModel.from_pretrained(peft_model_base, \n                                       '/kaggle/input/generative-ai-with-llms-lab-2/lab_2/peft-dialogue-summary-checkpoint-from-s3', \n                                       torch_dtype=torch.bfloat16,\n                                       is_trainable=False)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:57.120632Z","iopub.status.idle":"2024-11-28T07:10:57.121092Z","shell.execute_reply.started":"2024-11-28T07:10:57.120855Z","shell.execute_reply":"2024-11-28T07:10:57.120878Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The number of trainable parameters will be `0` due to `is_trainable=False` setting:","metadata":{"tags":[]}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:57.122762Z","iopub.status.idle":"2024-11-28T07:10:57.123167Z","shell.execute_reply.started":"2024-11-28T07:10:57.122956Z","shell.execute_reply":"2024-11-28T07:10:57.122976Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.3'></a>\n### 3.3 - Evaluate the Model Qualitatively (Human Evaluation)\n\nMake inferences for the same example as in sections [1.3](#1.3) and [2.3](#2.3), with the original model, fully fine-tuned and PEFT model.","metadata":{}},{"cell_type":"code","source":"peft_model = peft_model.to('cpu')\ninstruct_model = instruct_model.to('cpu')\noriginal_model = original_model.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-11-28T07:10:57.124097Z","iopub.status.idle":"2024-11-28T07:10:57.124626Z","shell.execute_reply.started":"2024-11-28T07:10:57.124393Z","shell.execute_reply":"2024-11-28T07:10:57.124416Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"index = 200\ndialogue = dataset['test'][index]['dialogue']\nbaseline_human_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\npeft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\npeft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\nprint(dash_line)\nprint(f'PEFT MODEL: {peft_model_text_output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:57.126227Z","iopub.status.idle":"2024-11-28T07:10:57.126543Z","shell.execute_reply.started":"2024-11-28T07:10:57.126400Z","shell.execute_reply":"2024-11-28T07:10:57.126415Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.4'></a>\n### 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\nPerform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). ","metadata":{}},{"cell_type":"code","source":"dialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    human_baseline_text_output = human_baseline_summaries[idx]\n    \n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\n    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\n    original_model_summaries.append(original_model_text_output)\n    instruct_model_summaries.append(instruct_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:57.128024Z","iopub.status.idle":"2024-11-28T07:10:57.128372Z","shell.execute_reply.started":"2024-11-28T07:10:57.128193Z","shell.execute_reply":"2024-11-28T07:10:57.128233Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"Compute ROUGE score for this subset of the data. ","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:57.130056Z","iopub.status.idle":"2024-11-28T07:10:57.130524Z","shell.execute_reply.started":"2024-11-28T07:10:57.130277Z","shell.execute_reply":"2024-11-28T07:10:57.130299Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Notice, that PEFT model results are not too bad, while the training process was much easier!","metadata":{}},{"cell_type":"markdown","source":"You already computed ROUGE score on the full dataset, after loading the results from the `data/dialogue-summary-training-results.csv` file. Load the values for the PEFT model now and check its performance compared to other models.","metadata":{}},{"cell_type":"code","source":"human_baseline_summaries = results['human_baseline_summaries'].values\noriginal_model_summaries = results['original_model_summaries'].values\ninstruct_model_summaries = results['instruct_model_summaries'].values\npeft_model_summaries     = results['peft_model_summaries'].values\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:57.132141Z","iopub.status.idle":"2024-11-28T07:10:57.132450Z","shell.execute_reply.started":"2024-11-28T07:10:57.132312Z","shell.execute_reply":"2024-11-28T07:10:57.132326Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The results show less of an improvement over full fine-tuning, but the benefits of PEFT typically outweigh the slightly-lower performance metrics.\n\nCalculate the improvement of PEFT over the original model:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:57.134028Z","iopub.status.idle":"2024-11-28T07:10:57.134359Z","shell.execute_reply.started":"2024-11-28T07:10:57.134176Z","shell.execute_reply":"2024-11-28T07:10:57.134190Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now calculate the improvement of PEFT over a full fine-tuned model:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-28T07:10:57.135580Z","iopub.status.idle":"2024-11-28T07:10:57.135885Z","shell.execute_reply.started":"2024-11-28T07:10:57.135742Z","shell.execute_reply":"2024-11-28T07:10:57.135757Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here you see a small percentage decrease in the ROUGE metrics vs. full fine-tuned. However, the training requires much less computing and memory resources (often just a single GPU).","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Source:\n - https://www.coursera.org/learn/generative-ai-with-llms\n - https://www.coursera.org/learn/generative-ai-with-llms/gradedLti/x0gc1/lab-2-fine-tune-a-generative-ai-model-for-dialogue-summarization\n \nhttps://creativecommons.org/licenses/by-sa/2.0/legalcode","metadata":{}}]}