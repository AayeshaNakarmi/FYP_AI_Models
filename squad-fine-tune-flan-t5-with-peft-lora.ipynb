{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6131005,"sourceType":"datasetVersion","datasetId":3515188},{"sourceId":160296,"sourceType":"datasetVersion","datasetId":72533},{"sourceId":4251,"sourceType":"modelInstanceVersion","modelInstanceId":3045,"modelId":575}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tune a Generative AI Model for Dialogue Summarization","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Here I am just sharing an interesting tutorial that was included as part of a course that I recently audited on Coursera.\n\nI thought that this tutorial did a great job of explaining parameter efficient fine-tuning methods (as well as providing examples for how to implement these methods).\n\n\nSource:\n - https://www.coursera.org/learn/generative-ai-with-llms\n - https://www.coursera.org/learn/generative-ai-with-llms/gradedLti/x0gc1/lab-2-fine-tune-a-generative-ai-model-for-dialogue-summarization\n\nhttps://creativecommons.org/licenses/by-sa/2.0/legalcode","metadata":{}},{"cell_type":"markdown","source":"In this notebook, you will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics.","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents","metadata":{}},{"cell_type":"markdown","source":"- [ 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM](#1)\n  - [ 1.1 - Set up Kernel and Required Dependencies](#1.1)\n  - [ 1.2 - Load Dataset and LLM](#1.2)\n  - [ 1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n- [ 2 - Perform Full Fine-Tuning](#2)\n  - [ 2.1 - Preprocess the Dialog-Summary Dataset](#2.1)\n  - [ 2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n  - [ 2.3 - Evaluate the Model Qualitatively (Human Evaluation)](#2.3)\n  - [ 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#2.4)\n- [ 3 - Perform Parameter Efficient Fine-Tuning (PEFT)](#3)\n  - [ 3.1 - Setup the PEFT/LoRA model for Fine-Tuning](#3.1)\n  - [ 3.2 - Train PEFT Adapter](#3.2)\n  - [ 3.3 - Evaluate the Model Qualitatively (Human Evaluation)](#3.3)\n  - [ 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#3.4)","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='1'></a>\n## 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM","metadata":{}},{"cell_type":"markdown","source":"<a name='1.1'></a>\n### 1.1 - Set up Kernel and Required Dependencies","metadata":{}},{"cell_type":"markdown","source":"Now install the required packages for the LLM and datasets.\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5JZ25vcmUgdGhlIHdhcm5pbmdzIGFuZCBlcnJvcnMsIGFsb25nIHdpdGggdGhlIG5vdGUgYWJvdXQgcmVzdGFydGluZyB0aGUga2VybmVsIGF0IHRoZSBlbmQuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>","metadata":{"tags":[]}},{"cell_type":"code","source":"%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.11.0 \\\n    evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    loralib==0.1.1 \\\n    peft==0.3.0 --quiet","metadata":{"tags":[],"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-11-26T12:37:51.277902Z","iopub.execute_input":"2024-11-26T12:37:51.278155Z","iopub.status.idle":"2024-11-26T12:39:55.583642Z","shell.execute_reply.started":"2024-11-26T12:37:51.278132Z","shell.execute_reply":"2024-11-26T12:39:55.582252Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.1.2)\nCollecting pip\n  Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.1.2\n    Uninstalling pip-23.1.2:\n      Successfully uninstalled pip-23.1.2\nSuccessfully installed pip-24.3.1\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"code","source":"!pip install awscli ","metadata":{"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:39:55.586098Z","iopub.execute_input":"2024-11-26T12:39:55.586535Z","iopub.status.idle":"2024-11-26T12:40:06.563430Z","shell.execute_reply.started":"2024-11-26T12:39:55.586494Z","shell.execute_reply":"2024-11-26T12:40:06.562516Z"}},"outputs":[{"name":"stdout","text":"Collecting awscli\n  Downloading awscli-1.36.10-py3-none-any.whl.metadata (11 kB)\nCollecting botocore==1.35.69 (from awscli)\n  Downloading botocore-1.35.69-py3-none-any.whl.metadata (5.7 kB)\nCollecting docutils<0.17,>=0.10 (from awscli)\n  Downloading docutils-0.16-py2.py3-none-any.whl.metadata (2.7 kB)\nCollecting s3transfer<0.11.0,>=0.10.0 (from awscli)\n  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: PyYAML<6.1,>=3.10 in /opt/conda/lib/python3.10/site-packages (from awscli) (6.0)\nRequirement already satisfied: colorama<0.4.7,>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from awscli) (0.4.6)\nCollecting rsa<4.8,>=3.1.2 (from awscli)\n  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore==1.35.69->awscli) (1.0.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore==1.35.69->awscli) (2.8.2)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore==1.35.69->awscli) (1.26.15)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.35.69->awscli) (1.16.0)\nDownloading awscli-1.36.10-py3-none-any.whl (4.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading botocore-1.35.69-py3-none-any.whl (13.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m152.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rsa-4.7.2-py3-none-any.whl (34 kB)\nDownloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\nInstalling collected packages: rsa, docutils, botocore, s3transfer, awscli\n  Attempting uninstall: rsa\n    Found existing installation: rsa 4.9\n    Uninstalling rsa-4.9:\n      Successfully uninstalled rsa-4.9\n  Attempting uninstall: docutils\n    Found existing installation: docutils 0.20.1\n    Uninstalling docutils-0.20.1:\n      Successfully uninstalled docutils-0.20.1\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.29.161\n    Uninstalling botocore-1.29.161:\n      Successfully uninstalled botocore-1.29.161\n  Attempting uninstall: s3transfer\n    Found existing installation: s3transfer 0.6.1\n    Uninstalling s3transfer-0.6.1:\n      Successfully uninstalled s3transfer-0.6.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.5.2 requires botocore<1.29.162,>=1.29.161, but you have botocore 1.35.69 which is incompatible.\nboto3 1.26.100 requires botocore<1.30.0,>=1.29.100, but you have botocore 1.35.69 which is incompatible.\nboto3 1.26.100 requires s3transfer<0.7.0,>=0.6.0, but you have s3transfer 0.10.4 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed awscli-1.36.10 botocore-1.35.69 docutils-0.16 rsa-4.7.2 s3transfer-0.10.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Import the necessary components. Some of them are new for this week, they will be discussed later in the notebook. ","metadata":{"tags":[]}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport evaluate\nimport pandas as pd\nimport numpy as np","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-26T12:40:06.565241Z","iopub.execute_input":"2024-11-26T12:40:06.565530Z","iopub.status.idle":"2024-11-26T12:40:19.251681Z","shell.execute_reply.started":"2024-11-26T12:40:06.565506Z","shell.execute_reply":"2024-11-26T12:40:19.250977Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"<a name='1.2'></a>\n### 1.2 - Load Dataset and LLM\n\nYou are going to continue experimenting with the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. ","metadata":{"tags":[]}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the SQuAD v2.0 dataset\ndataset = load_dataset(\"squad_v2\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-26T12:40:19.253502Z","iopub.execute_input":"2024-11-26T12:40:19.253761Z","iopub.status.idle":"2024-11-26T12:40:26.478045Z","shell.execute_reply.started":"2024-11-26T12:40:19.253737Z","shell.execute_reply":"2024-11-26T12:40:26.477135Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/8.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"382a878b1d764fa290bfc2f542c7e896"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset None/squad_v2 to /root/.cache/huggingface/datasets/parquet/squad_v2-ea29cd02ee439285/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"037367f3ea604a0b92dd53cc31251444"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/16.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e6d50d313d4ffa9b74b50554db1a4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dc9e726cb4f48e39091f251499c58ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5314514c081a4e1fa2e10aace8cedb54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/squad_v2-ea29cd02ee439285/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b3e048b5b4a4247aa5f61bc2c33b65f"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Preprocess the train and validation data\ndef preprocess_data(data):\n    articles = []\n    \n    # Iterate through each article in the dataset\n    for article in data:\n        context = article[\"context\"]  # context is a string for each article\n        question = article[\"question\"]  # question field\n        \n        # Store only context and question (as input-output pairs for QG)\n        articles.append({\n            'context': context,\n            'question': question\n        })\n    \n    return articles\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:40:26.478920Z","iopub.execute_input":"2024-11-26T12:40:26.479194Z","iopub.status.idle":"2024-11-26T12:40:26.484142Z","shell.execute_reply.started":"2024-11-26T12:40:26.479172Z","shell.execute_reply":"2024-11-26T12:40:26.483193Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(dataset['train'][0])  # Print the first element of the 'train' split to check the structure","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:40:26.485204Z","iopub.execute_input":"2024-11-26T12:40:26.485451Z","iopub.status.idle":"2024-11-26T12:40:26.495684Z","shell.execute_reply.started":"2024-11-26T12:40:26.485430Z","shell.execute_reply":"2024-11-26T12:40:26.494991Z"}},"outputs":[{"name":"stdout","text":"{'id': '56be85543aeaaa14008c9063', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train_data = preprocess_data(dataset['train'])\nval_data = preprocess_data(dataset['validation'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:40:26.496799Z","iopub.execute_input":"2024-11-26T12:40:26.497051Z","iopub.status.idle":"2024-11-26T12:40:34.041025Z","shell.execute_reply.started":"2024-11-26T12:40:26.497030Z","shell.execute_reply":"2024-11-26T12:40:34.039998Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Load the pre-trained [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer directly from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-base) of FLAN-T5. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by this model.","metadata":{"tags":[]}},{"cell_type":"code","source":"print(train_data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:40:34.042327Z","iopub.execute_input":"2024-11-26T12:40:34.042565Z","iopub.status.idle":"2024-11-26T12:40:34.047148Z","shell.execute_reply.started":"2024-11-26T12:40:34.042544Z","shell.execute_reply":"2024-11-26T12:40:34.046259Z"}},"outputs":[{"name":"stdout","text":"{'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?'}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Print the size of train_data and val_data\nprint(f\"Size of train_data: {len(train_data)}\")\nprint(f\"Size of val_data: {len(val_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:40:34.048276Z","iopub.execute_input":"2024-11-26T12:40:34.048628Z","iopub.status.idle":"2024-11-26T12:40:34.069717Z","shell.execute_reply.started":"2024-11-26T12:40:34.048592Z","shell.execute_reply":"2024-11-26T12:40:34.068875Z"}},"outputs":[{"name":"stdout","text":"Size of train_data: 130319\nSize of val_data: 11873\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# model_name='google/flan-t5-base'\nmodel_name='/kaggle/input/flan-t5/pytorch/base/4'\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-26T12:40:34.073466Z","iopub.execute_input":"2024-11-26T12:40:34.073748Z","iopub.status.idle":"2024-11-26T12:40:46.111397Z","shell.execute_reply.started":"2024-11-26T12:40:34.073724Z","shell.execute_reply":"2024-11-26T12:40:46.110642Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it. ","metadata":{"tags":[]}},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-26T12:40:46.112400Z","iopub.execute_input":"2024-11-26T12:40:46.112689Z","iopub.status.idle":"2024-11-26T12:40:46.119505Z","shell.execute_reply.started":"2024-11-26T12:40:46.112664Z","shell.execute_reply":"2024-11-26T12:40:46.118576Z"},"trusted":true},"outputs":[{"name":"stdout","text":"trainable model parameters: 247577856\nall model parameters: 247577856\npercentage of trainable model parameters: 100.00%\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(dataset.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:40:46.120541Z","iopub.execute_input":"2024-11-26T12:40:46.120791Z","iopub.status.idle":"2024-11-26T12:40:46.149022Z","shell.execute_reply.started":"2024-11-26T12:40:46.120765Z","shell.execute_reply":"2024-11-26T12:40:46.148005Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['train', 'validation'])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"<a name='1.3'></a>\n### 1.3 - Test the Model with Zero Shot Inferencing\n\nTest the model with the zero shot inferencing. You can see that the model struggles to summarize the dialogue compared to the baseline summary, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand.","metadata":{"tags":[]}},{"cell_type":"code","source":"index = 200\n\n# Use the 'validation' split (since 'test' does not exist)\ncontext = dataset['validation'][index]['context']\n\n# Update the prompt to focus on question generation based on the given context\nprompt = f\"\"\"\nGenerate a question based on the following context.\n\nContext:\n{context}\n\nQuestion:\n\"\"\"\n\n# Tokenize the prompt and feed it to the model for generation\ninputs = tokenizer(prompt, return_tensors='pt')\n\n# Generate the output (question) from the model\noutput = tokenizer.decode(\n    original_model.generate(\n        inputs[\"input_ids\"], \n        max_new_tokens=50,  # Adjust the number of tokens to control question length\n    )[0], \n    skip_special_tokens=True\n)\n\n# Print the output\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'MODEL GENERATED QUESTION:\\n{output}')\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-26T12:40:46.150280Z","iopub.execute_input":"2024-11-26T12:40:46.150578Z","iopub.status.idle":"2024-11-26T12:40:47.692347Z","shell.execute_reply.started":"2024-11-26T12:40:46.150554Z","shell.execute_reply":"2024-11-26T12:40:47.691280Z"},"trusted":true},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nGenerate a question based on the following context.\n\nContext:\nNormandy was the site of several important developments in the history of classical music in the 11th century. Fécamp Abbey and Saint-Evroul Abbey were centres of musical production and education. At Fécamp, under two Italian abbots, William of Volpiano and John of Ravenna, the system of denoting notes by letters was developed and taught. It is still the most common form of pitch representation in English- and German-speaking countries today. Also at Fécamp, the staff, around which neumes were oriented, was first developed and taught in the 11th century. Under the German abbot Isembard, La Trinité-du-Mont became a centre of musical composition.\n\nQuestion:\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATED QUESTION:\nWhat is the most common form of pitch representation in English- and German-speaking countries today?\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"<a name='2'></a>\n## 2 - Perform Full Fine-Tuning","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='2.1'></a>\n### 2.1 - Preprocess the Dialog-Summary Dataset\n\nYou need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Summarize the following conversation` and to the start of the summary with `Summary` as follows:\n\nTraining prompt (dialogue):\n```\nSummarize the following conversation.\n\n    Chris: This is his part of the conversation.\n    Antje: This is her part of the conversation.\n    \nSummary: \n```\n\nTraining response (summary):\n```\nBoth Chris and Antje participated in the conversation.\n```\n\nThen preprocess the prompt-response dataset into tokens and pull out their `input_ids` (1 per token).","metadata":{"tags":[]}},{"cell_type":"code","source":"def tokenize_data(data, tokenizer, max_input_length=512, max_target_length=64):\n    tokenized_data = []\n\n    for article in data:\n        context = article[\"context\"]  # Context from the dataset\n        question = article[\"question\"]  # The question is the target\n\n        # Tokenize context (input text) and question (target text) separately\n        inputs = tokenizer(context, padding=\"max_length\", truncation=True, max_length=max_input_length, return_tensors=\"pt\")\n        targets = tokenizer(question, padding=\"max_length\", truncation=True, max_length=max_target_length, return_tensors=\"pt\")\n\n        tokenized_data.append({\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n            \"labels\": targets[\"input_ids\"].squeeze()\n        })\n    \n    return tokenized_data\n\n# Tokenize the preprocessed data (train and validation)\ntrain_tokenized = tokenize_data(train_data, tokenizer)\nval_tokenized = tokenize_data(val_data, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:40:47.693532Z","iopub.execute_input":"2024-11-26T12:40:47.693813Z","iopub.status.idle":"2024-11-26T12:43:35.923368Z","shell.execute_reply.started":"2024-11-26T12:40:47.693790Z","shell.execute_reply":"2024-11-26T12:43:35.922567Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass QuestionGenerationDataset(Dataset):\n    def __init__(self, tokenized_data):\n        self.data = tokenized_data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# Create datasets\ntrain_dataset = QuestionGenerationDataset(train_tokenized)\nval_dataset = QuestionGenerationDataset(val_tokenized)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:43:35.924596Z","iopub.execute_input":"2024-11-26T12:43:35.925348Z","iopub.status.idle":"2024-11-26T12:43:35.930429Z","shell.execute_reply.started":"2024-11-26T12:43:35.925318Z","shell.execute_reply":"2024-11-26T12:43:35.929572Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Create DataLoaders for training and validation\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8)\n\n# Example of iterating through the training batches\nfor batch in train_dataloader:\n    print(batch)\n    break  # Just to inspect the first batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:43:35.931488Z","iopub.execute_input":"2024-11-26T12:43:35.931736Z","iopub.status.idle":"2024-11-26T12:43:35.958400Z","shell.execute_reply.started":"2024-11-26T12:43:35.931714Z","shell.execute_reply":"2024-11-26T12:43:35.957446Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[1179,  189, 1887,  ...,    0,    0,    0],\n        [  71, 6736,    9,  ...,    0,    0,    0],\n        [ 461, 2307, 1515,  ...,    0,    0,    0],\n        ...,\n        [ 276,    2,  413,  ...,    0,    0,    0],\n        [2840,    9,    7,  ...,    0,    0,    0],\n        [  37, 2793,   13,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 4073,  2815,    65,   165,  1025,  1375,   344, 18516,   107,  6916,\n            11,  4738,   189,  6916,    58,     1,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [  571,   410,     3,    88,   857,  1850,   764,   139,     8,   296,\n            58,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [ 2840,   410,  1534,    32,  4603,    26,  2444, 15884,    24,     3,\n            88,   133,  7770,    15,    16,     8,  9677,  4129,    58,     1,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [  363,  2306,   410,  5428, 15822,  1883,    16,  1186,  2735,    58,\n             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [  363,    47,  5974,  1406,    15,    26,    30,     8,  2605,    13,\n         11337,   113,   646,     8,   616,    58,     1,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [  571,   186,  1951,   263,    95,     8,  1622,  4741,    58,     1,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [  363,   405,     3,     9, 22213,  5932,  2862,    58,     1,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0],\n        [  363,   103,     3, 24702,   688,  1664,   169,    12,   428,    72,\n          4798,   738,    12,    70,  6061,    58,     1,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0]])}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_function(example):\n    start_prompt = 'Summarize the following conversation.\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    \n    return example\n\n# The dataset actually contains 3 diff splits: train, validation, test.\n# The tokenize_function code is handling all data across all splits in batches.\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:21:29.061606Z","iopub.execute_input":"2023-07-13T20:21:29.063116Z","iopub.status.idle":"2023-07-13T20:21:30.521036Z","shell.execute_reply.started":"2023-07-13T20:21:29.06307Z","shell.execute_reply":"2023-07-13T20:21:30.520148Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To save some time in the lab, you will subsample the dataset:","metadata":{"tags":[]}},{"cell_type":"code","source":"tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:21:30.522725Z","iopub.execute_input":"2023-07-13T20:21:30.523148Z","iopub.status.idle":"2023-07-13T20:21:31.607702Z","shell.execute_reply.started":"2023-07-13T20:21:30.523108Z","shell.execute_reply":"2023-07-13T20:21:31.606611Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check the shapes of all three parts of the dataset:","metadata":{"tags":[]}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Print the size (length) of the datasets\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(val_dataset)}\")","metadata":{"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The output dataset is ready for fine-tuning.","metadata":{}},{"cell_type":"markdown","source":"<a name='2.2'></a>\n### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n\nNow utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment.","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nfrom transformers import Trainer, TrainingArguments\n\n# Disable Weights & Biases logging (optional)\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Define the output directory for saving model checkpoints\noutput_dir = f'./flan-t5-squad-training-{str(int(time.time()))}'\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-5,\n    num_train_epochs=7,  # You can adjust this as necessary\n    weight_decay=0.01,\n    logging_steps=500,  # Log every 500 steps\n    save_steps=500,  # Save checkpoints every 500 steps\n    evaluation_strategy=\"steps\",  # Evaluate the model during training\n    eval_steps=500,  # Evaluate every 500 steps\n    max_steps=-1,  # If set to -1, training will run for the specified epochs\n    per_device_train_batch_size=8,  # Adjust batch size based on your memory\n    per_device_eval_batch_size=8,  # Adjust batch size for evaluation\n    report_to=None,  # Disable reporting to external services\n    fp16=True,  # Enable mixed precision training (FP16)\n)\n\n\n\n\n# Initialize the Trainer with model, training arguments, and datasets\ntrainer = Trainer(\n    model=original_model,  # Make sure `original_model` is the FLAN-T5 model\n    args=training_args,\n    train_dataset=train_dataset,  # Use your tokenized training dataset\n    eval_dataset=val_dataset,  # Use your tokenized validation dataset\n)\n\n# Start training\ntrainer.train()\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-26T12:00:23.509813Z","iopub.status.idle":"2024-11-26T12:00:23.510125Z","shell.execute_reply.started":"2024-11-26T12:00:23.509979Z","shell.execute_reply":"2024-11-26T12:00:23.509993Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Start training process...\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5Zb3UgY2FuIHNhZmVseSBpZ25vcmUgdGhlIHdhcm5pbmcgbWVzc2FnZXMuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>","metadata":{"tags":[]}},{"cell_type":"code","source":"trainer.train()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:21:33.360923Z","iopub.execute_input":"2023-07-13T20:21:33.361311Z","iopub.status.idle":"2023-07-13T20:21:35.19071Z","shell.execute_reply.started":"2023-07-13T20:21:33.361275Z","shell.execute_reply":"2023-07-13T20:21:35.189743Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Training a fully fine-tuned version of the model would take a few hours on a GPU. To save time, download a checkpoint of the fully fine-tuned model to use in the rest of this notebook. This fully fine-tuned model will also be referred to as the **instruct model** in this lab.","metadata":{}},{"cell_type":"code","source":"# !aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:21:35.196329Z","iopub.execute_input":"2023-07-13T20:21:35.19719Z","iopub.status.idle":"2023-07-13T20:21:35.202321Z","shell.execute_reply.started":"2023-07-13T20:21:35.197153Z","shell.execute_reply":"2023-07-13T20:21:35.200829Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The size of the downloaded instruct model is approximately 1GB.","metadata":{"tags":[]}},{"cell_type":"code","source":"# !ls -alh /kaggle/working/flan-dialogue-summary-checkpoint/pytorch_model.bin","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:21:35.203744Z","iopub.execute_input":"2023-07-13T20:21:35.204253Z","iopub.status.idle":"2023-07-13T20:21:35.212671Z","shell.execute_reply.started":"2023-07-13T20:21:35.204216Z","shell.execute_reply":"2023-07-13T20:21:35.211699Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:","metadata":{"tags":[]}},{"cell_type":"code","source":"original_model = original_model.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T20:21:35.213892Z","iopub.execute_input":"2023-07-13T20:21:35.21432Z","iopub.status.idle":"2023-07-13T20:21:36.031366Z","shell.execute_reply.started":"2023-07-13T20:21:35.214282Z","shell.execute_reply":"2023-07-13T20:21:36.030049Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/input/generative-ai-with-llms-lab-2/lab_2/flan-dialogue-summary-checkpoint/\", torch_dtype=torch.bfloat16).to('cpu')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:21:36.033215Z","iopub.execute_input":"2023-07-13T20:21:36.033886Z","iopub.status.idle":"2023-07-13T20:21:41.952432Z","shell.execute_reply.started":"2023-07-13T20:21:36.033844Z","shell.execute_reply":"2023-07-13T20:21:41.951371Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='2.3'></a>\n### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)\n\nAs with many GenAI applications, a qualitative approach where you ask yourself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), you can see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model.","metadata":{}},{"cell_type":"code","source":"index = 200\ndialogue = dataset['test'][index]['dialogue']\nhuman_baseline_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:21:41.954075Z","iopub.execute_input":"2023-07-13T20:21:41.95447Z","iopub.status.idle":"2023-07-13T20:21:53.33742Z","shell.execute_reply.started":"2023-07-13T20:21:41.954434Z","shell.execute_reply":"2023-07-13T20:21:53.336152Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='2.4'></a>\n### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n\nThe [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning.","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load('rouge')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:21:53.338998Z","iopub.execute_input":"2023-07-13T20:21:53.339981Z","iopub.status.idle":"2023-07-13T20:21:54.625203Z","shell.execute_reply.started":"2023-07-13T20:21:53.339941Z","shell.execute_reply":"2023-07-13T20:21:54.624202Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Generate the outputs for the sample of the test dataset (only 10 dialogues and summaries to save time), and save the results.","metadata":{}},{"cell_type":"code","source":"dialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\n\nfor _, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n    original_model_summaries.append(original_model_text_output)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n    instruct_model_summaries.append(instruct_model_text_output)\n    \nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\ndf","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:21:54.626655Z","iopub.execute_input":"2023-07-13T20:21:54.627036Z","iopub.status.idle":"2023-07-13T20:23:39.954482Z","shell.execute_reply.started":"2023-07-13T20:21:54.626997Z","shell.execute_reply":"2023-07-13T20:23:39.953316Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluate the models computing ROUGE metrics. Notice the improvement in the results!","metadata":{"tags":[]}},{"cell_type":"code","source":"original_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:23:39.956885Z","iopub.execute_input":"2023-07-13T20:23:39.957966Z","iopub.status.idle":"2023-07-13T20:23:40.47742Z","shell.execute_reply.started":"2023-07-13T20:23:39.95793Z","shell.execute_reply":"2023-07-13T20:23:40.476028Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The file `data/dialogue-summary-training-results.csv` contains a pre-populated list of all model results which you can use to evaluate on a larger section of data. Let's do that for each of the models:","metadata":{}},{"cell_type":"code","source":"# results = pd.read_csv(\"data/dialogue-summary-training-results.csv\")\nresults = pd.read_csv(\"/kaggle/input/generative-ai-with-llms-lab-2/lab_2/data/dialogue-summary-training-results.csv\")\n\nhuman_baseline_summaries = results['human_baseline_summaries'].values\noriginal_model_summaries = results['original_model_summaries'].values\ninstruct_model_summaries = results['instruct_model_summaries'].values\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:23:40.479108Z","iopub.execute_input":"2023-07-13T20:23:40.479819Z","iopub.status.idle":"2023-07-13T20:23:49.79537Z","shell.execute_reply.started":"2023-07-13T20:23:40.479756Z","shell.execute_reply":"2023-07-13T20:23:49.794124Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The results show substantial improvement in all ROUGE metrics:","metadata":{"tags":[]}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\")\n\nimprovement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(instruct_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:23:49.79719Z","iopub.execute_input":"2023-07-13T20:23:49.797616Z","iopub.status.idle":"2023-07-13T20:23:49.807885Z","shell.execute_reply.started":"2023-07-13T20:23:49.797575Z","shell.execute_reply":"2023-07-13T20:23:49.804819Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3'></a>\n## 3 - Perform Parameter Efficient Fine-Tuning (PEFT)\n\nNow, let's perform **Parameter Efficient Fine-Tuning (PEFT)** fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon. \n\nPEFT is a generic term that includes **Low-Rank Adaptation (LoRA)** and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained “LoRA adapter” emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n\nThat said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases.","metadata":{}},{"cell_type":"markdown","source":"<a name='3.1'></a>\n### 3.1 - Setup the PEFT/LoRA model for Fine-Tuning\n\nYou need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (`r`) hyper-parameter, which defines the rank/dimension of the adapter to be trained.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\n# Define the PEFT configuration (LoRA)\nlora_config = LoraConfig(\n    r=16,  # Rank (Optimal range: 8-32, lower for fewer parameters, higher for more)\n    lora_alpha=16,  # Scaling factor (typically between 8 and 32, balance between efficiency and model adaptation)\n    target_modules=[\"q\", \"v\"],  # Query and Value matrices for LoRA adaptation (default)\n    lora_dropout=0.1,  # Dropout to prevent overfitting, typically 0.1-0.2\n    bias=\"none\",  # No bias updates\n    task_type=TaskType.SEQ_2_SEQ_LM  # For sequence-to-sequence models like FLAN-T5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:43:36.543797Z","iopub.execute_input":"2024-11-26T12:43:36.544041Z","iopub.status.idle":"2024-11-26T12:43:36.548626Z","shell.execute_reply.started":"2024-11-26T12:43:36.544018Z","shell.execute_reply":"2024-11-26T12:43:36.547815Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Apply LoRA to your model\npeft_model = get_peft_model(original_model, lora_config)\n\n# Check the number of trainable parameters\ndef print_number_of_trainable_model_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Trainable parameters in PEFT model: {print_number_of_trainable_model_parameters(peft_model)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:43:36.549756Z","iopub.execute_input":"2024-11-26T12:43:36.550082Z","iopub.status.idle":"2024-11-26T12:43:36.608972Z","shell.execute_reply.started":"2024-11-26T12:43:36.550052Z","shell.execute_reply":"2024-11-26T12:43:36.608088Z"}},"outputs":[{"name":"stdout","text":"Trainable parameters in PEFT model: 1769472\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"Add LoRA adapter layers/parameters to the original LLM to be trained.","metadata":{"tags":[]}},{"cell_type":"code","source":"peft_model = get_peft_model(original_model, \n                            lora_config)\nprint(print_number_of_trainable_model_parameters(peft_model))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-26T12:43:36.610173Z","iopub.execute_input":"2024-11-26T12:43:36.610776Z","iopub.status.idle":"2024-11-26T12:43:36.662681Z","shell.execute_reply.started":"2024-11-26T12:43:36.610743Z","shell.execute_reply":"2024-11-26T12:43:36.661856Z"},"trusted":true},"outputs":[{"name":"stdout","text":"1769472\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Define the output directory for saving model checkpoints\noutput_dir = f'/kaggle/working/peft-flan-t5-squad-{str(int(time.time()))}'\n\n# Define TrainingArguments for PEFT fine-tuning\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=2e-5,  # Optimal learning rate for PEFT (higher than standard fine-tuning, but still small)\n    num_train_epochs=7,  # More epochs for better adaptation to the task\n    logging_steps=500,  # Log every 500 steps (adjust based on the dataset size)\n    evaluation_strategy=\"steps\",  # Evaluate every few steps\n    eval_steps=500,  # Evaluate every 500 steps (adjust for your dataset size)\n    save_steps=500,  # Save the model every 500 steps\n    per_device_train_batch_size=16,  # Batch size for training, adjust depending on available memory\n    per_device_eval_batch_size=16,  # Batch size for evaluation, adjust based on memory\n    warmup_steps=1000,  # Number of steps for learning rate warmup\n    weight_decay=0.01,  # Weight decay to prevent overfitting\n    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps to simulate larger batch size\n    fp16=True,  # Use mixed precision for faster training\n    report_to=None,  # Disable reporting to external services (you can enable if needed)\n    save_total_limit=3,  # Limit the number of saved checkpoints to avoid filling up disk space\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:43:36.663679Z","iopub.execute_input":"2024-11-26T12:43:36.663930Z","iopub.status.idle":"2024-11-26T12:43:36.720848Z","shell.execute_reply.started":"2024-11-26T12:43:36.663908Z","shell.execute_reply":"2024-11-26T12:43:36.720102Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Initialize the Trainer with model, training arguments, and datasets\npeft_trainer = Trainer(\n    model=peft_model,  # Make sure `original_model` is the FLAN-T5 model\n    args=peft_training_args,\n    train_dataset=train_dataset,  # Use your tokenized training dataset\n    eval_dataset=val_dataset,  # Use your tokenized validation dataset\n)\n\n# Start training\npeft_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T12:43:36.721884Z","iopub.execute_input":"2024-11-26T12:43:36.722161Z","iopub.status.idle":"2024-11-26T13:35:15.915349Z","shell.execute_reply.started":"2024-11-26T12:43:36.722138Z","shell.execute_reply":"2024-11-26T13:35:15.913884Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241126_124425-2sh79xla</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/aayeshanakarmi-london-metropolitan-university/huggingface/runs/2sh79xla' target=\"_blank\">silver-dragon-2</a></strong> to <a href='https://wandb.ai/aayeshanakarmi-london-metropolitan-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/aayeshanakarmi-london-metropolitan-university/huggingface' target=\"_blank\">https://wandb.ai/aayeshanakarmi-london-metropolitan-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/aayeshanakarmi-london-metropolitan-university/huggingface/runs/2sh79xla' target=\"_blank\">https://wandb.ai/aayeshanakarmi-london-metropolitan-university/huggingface/runs/2sh79xla</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1083' max='28504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1083/28504 50:13 < 21:13:49, 0.36 it/s, Epoch 0.27/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.000000</td>\n      <td>nan</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m10\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m)                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m# Start training\u001b[0m                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m10 peft_trainer.train()                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1633\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1630 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1631 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1632 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1633 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1634 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1635 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1636 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1904\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1901 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1902 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1903 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1904 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1905 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1906 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m is_torch_tpu_available()                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1907 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[95mand\u001b[0m (torch.isnan(tr_loss_step) \u001b[95mor\u001b[0m torch.isinf(tr_loss_step))          \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mKeyboardInterrupt\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>)                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 # Start training</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>10 peft_trainer.train()                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1633</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1630 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1631 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1632 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1633 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1634 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1635 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1636 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1904</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1901 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1902 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1903 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1904 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1905 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1906 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> is_torch_tpu_available()                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1907 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> (torch.isnan(tr_loss_step) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> torch.isinf(tr_loss_step))          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n</pre>\n"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"<a name='3.2'></a>\n### 3.2 - Train PEFT Adapter\n\nDefine training arguments and create `Trainer` instance.","metadata":{"tags":[]}},{"cell_type":"code","source":"output_dir = f'/kaggle/working/peft-dialogue-summary-training-{str(int(time.time()))}'\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n    num_train_epochs=1,\n    logging_steps=1,\n    max_steps=1    \n)\n    \npeft_trainer = Trainer(\n    model=peft_model,\n    args=peft_training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:23:50.536544Z","iopub.execute_input":"2023-07-13T20:23:50.538048Z","iopub.status.idle":"2023-07-13T20:23:50.890547Z","shell.execute_reply.started":"2023-07-13T20:23:50.53801Z","shell.execute_reply":"2023-07-13T20:23:50.889575Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now everything is ready to train the PEFT adapter and save the model.\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>","metadata":{}},{"cell_type":"code","source":"peft_trainer.train()\n\npeft_model_path=\"/kaggle/working/peft-dialogue-summary-checkpoint-local\"\n\npeft_trainer.model.save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:23:50.891972Z","iopub.execute_input":"2023-07-13T20:23:50.89233Z","iopub.status.idle":"2023-07-13T20:23:53.111493Z","shell.execute_reply.started":"2023-07-13T20:23:50.892303Z","shell.execute_reply":"2023-07-13T20:23:53.110456Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"That training was performed on a subset of data. To load a fully trained PEFT model, read a checkpoint of a PEFT model from S3.","metadata":{"tags":[]}},{"cell_type":"code","source":"# !aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ /kaggle/working/peft-dialogue-summary-checkpoint-from-s3/ ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:23:53.112829Z","iopub.execute_input":"2023-07-13T20:23:53.113218Z","iopub.status.idle":"2023-07-13T20:23:53.119207Z","shell.execute_reply.started":"2023-07-13T20:23:53.11318Z","shell.execute_reply":"2023-07-13T20:23:53.117459Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check that the size of this model is much less than the original LLM:","metadata":{"tags":[]}},{"cell_type":"code","source":"# !ls -al /kaggle/working/peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:23:53.12103Z","iopub.execute_input":"2023-07-13T20:23:53.121497Z","iopub.status.idle":"2023-07-13T20:23:53.127987Z","shell.execute_reply.started":"2023-07-13T20:23:53.121464Z","shell.execute_reply":"2023-07-13T20:23:53.126769Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Prepare this model by adding an adapter to the original FLAN-T5 model. You are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set `is_trainable=True`.","metadata":{"tags":[]}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\npeft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/input/flan-t5/pytorch/base/4\", torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/flan-t5/pytorch/base/4\")\n\npeft_model = PeftModel.from_pretrained(peft_model_base, \n                                       '/kaggle/input/generative-ai-with-llms-lab-2/lab_2/peft-dialogue-summary-checkpoint-from-s3', \n                                       torch_dtype=torch.bfloat16,\n                                       is_trainable=False)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:23:53.129546Z","iopub.execute_input":"2023-07-13T20:23:53.129927Z","iopub.status.idle":"2023-07-13T20:23:58.99049Z","shell.execute_reply.started":"2023-07-13T20:23:53.129893Z","shell.execute_reply":"2023-07-13T20:23:58.989442Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The number of trainable parameters will be `0` due to `is_trainable=False` setting:","metadata":{"tags":[]}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:23:58.992087Z","iopub.execute_input":"2023-07-13T20:23:58.992666Z","iopub.status.idle":"2023-07-13T20:23:59.005264Z","shell.execute_reply.started":"2023-07-13T20:23:58.992628Z","shell.execute_reply":"2023-07-13T20:23:59.003924Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.3'></a>\n### 3.3 - Evaluate the Model Qualitatively (Human Evaluation)\n\nMake inferences for the same example as in sections [1.3](#1.3) and [2.3](#2.3), with the original model, fully fine-tuned and PEFT model.","metadata":{}},{"cell_type":"code","source":"peft_model = peft_model.to('cpu')\ninstruct_model = instruct_model.to('cpu')\noriginal_model = original_model.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T20:23:59.007049Z","iopub.execute_input":"2023-07-13T20:23:59.007411Z","iopub.status.idle":"2023-07-13T20:23:59.541961Z","shell.execute_reply.started":"2023-07-13T20:23:59.007377Z","shell.execute_reply":"2023-07-13T20:23:59.540843Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"index = 200\ndialogue = dataset['test'][index]['dialogue']\nbaseline_human_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\npeft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\npeft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\nprint(dash_line)\nprint(f'PEFT MODEL: {peft_model_text_output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:23:59.543736Z","iopub.execute_input":"2023-07-13T20:23:59.544195Z","iopub.status.idle":"2023-07-13T20:24:22.958391Z","shell.execute_reply.started":"2023-07-13T20:23:59.544144Z","shell.execute_reply":"2023-07-13T20:24:22.957273Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.4'></a>\n### 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\nPerform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). ","metadata":{}},{"cell_type":"code","source":"dialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    human_baseline_text_output = human_baseline_summaries[idx]\n    \n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\n    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\n    original_model_summaries.append(original_model_text_output)\n    instruct_model_summaries.append(instruct_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:24:22.960065Z","iopub.execute_input":"2023-07-13T20:24:22.96048Z","iopub.status.idle":"2023-07-13T20:27:21.206861Z","shell.execute_reply.started":"2023-07-13T20:24:22.960443Z","shell.execute_reply":"2023-07-13T20:27:21.205871Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"Compute ROUGE score for this subset of the data. ","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:27:21.208644Z","iopub.execute_input":"2023-07-13T20:27:21.213564Z","iopub.status.idle":"2023-07-13T20:27:23.100951Z","shell.execute_reply.started":"2023-07-13T20:27:21.213524Z","shell.execute_reply":"2023-07-13T20:27:23.099877Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Notice, that PEFT model results are not too bad, while the training process was much easier!","metadata":{}},{"cell_type":"markdown","source":"You already computed ROUGE score on the full dataset, after loading the results from the `data/dialogue-summary-training-results.csv` file. Load the values for the PEFT model now and check its performance compared to other models.","metadata":{}},{"cell_type":"code","source":"human_baseline_summaries = results['human_baseline_summaries'].values\noriginal_model_summaries = results['original_model_summaries'].values\ninstruct_model_summaries = results['instruct_model_summaries'].values\npeft_model_summaries     = results['peft_model_summaries'].values\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:27:23.102655Z","iopub.execute_input":"2023-07-13T20:27:23.103369Z","iopub.status.idle":"2023-07-13T20:27:37.745072Z","shell.execute_reply.started":"2023-07-13T20:27:23.103328Z","shell.execute_reply":"2023-07-13T20:27:37.743963Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The results show less of an improvement over full fine-tuning, but the benefits of PEFT typically outweigh the slightly-lower performance metrics.\n\nCalculate the improvement of PEFT over the original model:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:27:37.74663Z","iopub.execute_input":"2023-07-13T20:27:37.747121Z","iopub.status.idle":"2023-07-13T20:27:37.754334Z","shell.execute_reply.started":"2023-07-13T20:27:37.747085Z","shell.execute_reply":"2023-07-13T20:27:37.753275Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now calculate the improvement of PEFT over a full fine-tuned model:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2023-07-13T20:27:37.75583Z","iopub.execute_input":"2023-07-13T20:27:37.756462Z","iopub.status.idle":"2023-07-13T20:27:37.768002Z","shell.execute_reply.started":"2023-07-13T20:27:37.756426Z","shell.execute_reply":"2023-07-13T20:27:37.767Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here you see a small percentage decrease in the ROUGE metrics vs. full fine-tuned. However, the training requires much less computing and memory resources (often just a single GPU).","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Source:\n - https://www.coursera.org/learn/generative-ai-with-llms\n - https://www.coursera.org/learn/generative-ai-with-llms/gradedLti/x0gc1/lab-2-fine-tune-a-generative-ai-model-for-dialogue-summarization\n \nhttps://creativecommons.org/licenses/by-sa/2.0/legalcode","metadata":{}}]}