{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10063570,"sourceType":"datasetVersion","datasetId":6201861},{"sourceId":85986,"sourceType":"modelInstanceVersion","modelInstanceId":72246,"modelId":78150},{"sourceId":178402,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":151979,"modelId":174433}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import keras_nlp\n\n# Load the model architecture\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-01T13:11:11.222645Z","iopub.execute_input":"2024-12-01T13:11:11.223423Z","iopub.status.idle":"2024-12-01T13:12:49.781569Z","shell.execute_reply.started":"2024-12-01T13:11:11.223389Z","shell.execute_reply":"2024-12-01T13:12:49.780349Z"}},"outputs":[{"name":"stderr","text":"normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Path to your weights file\nweights_path = \"/kaggle/input/gemma-quizard-weights/keras/default/1/QA-Gemma-Quizard.weights.h5\"\n\n# Load the weights\ngemma_lm.load_weights(weights_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T13:12:49.784207Z","iopub.execute_input":"2024-12-01T13:12:49.785011Z","iopub.status.idle":"2024-12-01T13:13:57.923188Z","shell.execute_reply.started":"2024-12-01T13:12:49.784955Z","shell.execute_reply":"2024-12-01T13:13:57.922155Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 210 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T13:13:57.924436Z","iopub.execute_input":"2024-12-01T13:13:57.924725Z","iopub.status.idle":"2024-12-01T13:14:01.950224Z","shell.execute_reply.started":"2024-12-01T13:13:57.924698Z","shell.execute_reply":"2024-12-01T13:14:01.949355Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T13:14:01.952106Z","iopub.execute_input":"2024-12-01T13:14:01.952652Z","iopub.status.idle":"2024-12-01T13:14:01.964968Z","shell.execute_reply.started":"2024-12-01T13:14:01.952621Z","shell.execute_reply":"2024-12-01T13:14:01.964110Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"template = \"\"\"Instruction: Generate an answer to the question using the provided context.  Provide the answer only without including any additional content.\"\n\nContext:\n{Context}\n\nQuestion:\n{Question}\n\nResponse:\n{Answer}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T13:14:01.966052Z","iopub.execute_input":"2024-12-01T13:14:01.966351Z","iopub.status.idle":"2024-12-01T13:14:01.971381Z","shell.execute_reply.started":"2024-12-01T13:14:01.966324Z","shell.execute_reply":"2024-12-01T13:14:01.970463Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"prompt = template.format(\n    Context=\"The Great Barrier Reef is the world's largest coral reef system, located in the Coral Sea, off the coast of Queensland, Australia. It is composed of over 2,900 individual reefs and 900 islands stretching over 2,300 kilometers. The reef is known for its biodiversity, hosting countless marine species, and is a popular destination for snorkeling and diving enthusiasts. However, it faces threats from climate change, overfishing, and pollution.\",\n    Question=\"What are some of the major threats faced by the Great Barrier Reef?\",\n    Answer=\"\"\n)\n\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T13:14:01.972464Z","iopub.execute_input":"2024-12-01T13:14:01.972719Z","iopub.status.idle":"2024-12-01T13:14:54.446457Z","shell.execute_reply.started":"2024-12-01T13:14:01.972693Z","shell.execute_reply":"2024-12-01T13:14:54.445495Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1733058859.948731      68 service.cc:145] XLA service 0x79f807f01980 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1733058859.950782      68 service.cc:153]   StreamExecutor device (0): Host, Default Version\nI0000 00:00:1733058860.299480      67 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Instruction: Generate an answer to the question using the provided context.  Provide the answer only without including any additional content.\"\n\nContext:\nThe Great Barrier Reef is the world's largest coral reef system, located in the Coral Sea, off the coast of Queensland, Australia. It is composed of over 2,900 individual reefs and 900 islands stretching over 2,300 kilometers. The reef is known for its biodiversity, hosting countless marine species, and is a popular destination for snorkeling and diving enthusiasts. However, it faces threats from climate change, overfishing, and pollution.\n\nQuestion:\nWhat are some of the major threats faced by the Great Barrier Reef?\n\nResponse:\n\nThe major threats to the Great Barrier Reef include climate change, overfishing, and pollution, which have led to coral bleaching and habitat degradation, impacting its biodiversity and ecological balance.\n\n\"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"prompt = template.format(\n    Context=\"After training a machine translation model on a large dataset, the BLEU score was calculated to evaluate the quality of the model's translations compared to a set of human-generated reference translations. The model's output was compared to the reference texts using n-gram overlap, and a brevity penalty was applied to avoid overly short translations. The BLEU score was found to be 0.75, which indicates that the model's translations closely align with the reference texts.\",\n    Question=\"What is the BLEU score of the model's output as calculated during evaluation?\",\n    Answer=\"\"\n)\n\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T13:14:54.447717Z","iopub.execute_input":"2024-12-01T13:14:54.447993Z","iopub.status.idle":"2024-12-01T13:15:38.711033Z","shell.execute_reply.started":"2024-12-01T13:14:54.447967Z","shell.execute_reply":"2024-12-01T13:15:38.710046Z"}},"outputs":[{"name":"stdout","text":"Instruction: Generate an answer to the question using the provided context.  Provide the answer only without including any additional content.\"\n\nContext:\nAfter training a machine translation model on a large dataset, the BLEU score was calculated to evaluate the quality of the model's translations compared to a set of human-generated reference translations. The model's output was compared to the reference texts using n-gram overlap, and a brevity penalty was applied to avoid overly short translations. The BLEU score was found to be 0.75, which indicates that the model's translations closely align with the reference texts.\n\nQuestion:\nWhat is the BLEU score of the model's output as calculated during evaluation?\n\nResponse:\n\nThe BLEU score was 0.75, indicating that the model's translations were reasonably accurate and aligned with the reference texts.\\\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\n# Retrieve the Hugging Face API token from Kaggle secrets\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T13:16:48.509414Z","iopub.execute_input":"2024-12-01T13:16:48.509826Z","iopub.status.idle":"2024-12-01T13:16:48.716949Z","shell.execute_reply.started":"2024-12-01T13:16:48.509761Z","shell.execute_reply":"2024-12-01T13:16:48.715837Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"!pip install PyPDF2 sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T13:18:08.327240Z","iopub.execute_input":"2024-12-01T13:18:08.328043Z","iopub.status.idle":"2024-12-01T13:18:18.931138Z","shell.execute_reply.started":"2024-12-01T13:18:08.328011Z","shell.execute_reply":"2024-12-01T13:18:18.929776Z"}},"outputs":[{"name":"stdout","text":"Collecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0+cpu)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: PyPDF2, sentence-transformers\nSuccessfully installed PyPDF2-3.0.1 sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import PyPDF2\nimport re\nimport nltk\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nimport keras_nlp\n\n# Download the Punkt tokenizer for sentence splitting (only need to run once)\nnltk.download('punkt')\n\n# Function to extract text from PDF\ndef extract_text_from_pdf(pdf_path):\n    try:\n        with open(pdf_path, 'rb') as file:\n            reader = PyPDF2.PdfReader(file)\n            text = \"\"\n            for page_num in range(len(reader.pages)):\n                page = reader.pages[page_num]\n                text += page.extract_text() or \"\"\n        return text\n    except Exception as e:\n        print(f\"Error extracting text from PDF: {e}\")\n        return \"\"\n\n# Function to exclude references section\ndef exclude_references(text):\n    match = re.search(r'(References|Bibliography|Works Cited)', text, re.IGNORECASE)\n    if match:\n        text = text[:match.start()]\n    return text\n\n# Function to split text into sentences\ndef split_text_into_sentences(text):\n    return nltk.tokenize.sent_tokenize(text)\n\n# Load the pre-trained model and tokenizer for question generation\nquestion_model_name = \"aayeshanakarmi/T5-QG-finetuned-squad\"\nquestion_tokenizer = AutoTokenizer.from_pretrained(question_model_name, use_auth_token=hf_token)\nquestion_model = AutoModelForSeq2SeqLM.from_pretrained(question_model_name, use_auth_token=hf_token)\n\n# Load SentenceTransformer model for semantic similarity\nsentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Load the Gemma model for answer generation\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")\nweights_path = \"/kaggle/input/gemma-quizard-weights/keras/default/1/QA-Gemma-Quizard.weights.h5\"  # Update to actual path\ngemma_lm.load_weights(weights_path)\n\n# Configure the sampler for the Gemma model\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\n\n# Template for answer generation\nanswer_template = \"\"\"Instruction: Generate an answer to the question using the provided context. Provide the answer only without including any additional content.\n\nContext:\n{Context}\n\nQuestion:\n{Question}\n\nResponse:\n\"\"\"\n\n# Function to generate questions from a group of sentences\ndef generate_questions_from_group(sentence_group):\n    paragraph = \" \".join(sentence_group)\n    inputs = question_tokenizer.encode(paragraph, return_tensors=\"pt\", truncation=True, max_length=512)\n    outputs = question_model.generate(inputs, max_length=150, num_beams=4, early_stopping=True)\n    question = question_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    if question.strip() == \"\":\n        question = \"No valid question generated.\"\n    \n    return question\n\n# Function to generate answers using the Gemma model\ndef generate_answer_with_gemma(context, question):\n    prompt = answer_template.format(Context=context, Question=question)\n    response = gemma_lm.generate(prompt, max_length=256)\n    return response[0]  # Return the generated answer\n\n# Function to generate unique questions and answers\ndef generate_unique_questions_and_answers(sentences):\n    # Create embeddings for all sentences\n    embeddings = sentence_model.encode(sentences)\n    qa_pairs = []\n\n    # Group sentences based on similarity and generate questions and answers\n    for i in range(len(embeddings)):\n        current_group = [sentences[i]]\n        for j in range(i + 1, len(embeddings)):\n            similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n            if similarity > 0.7:\n                current_group.append(sentences[j])\n        \n        # Generate question and answer if the group is large enough\n        if len(current_group) > 1:\n            question = generate_questions_from_group(current_group)\n            context = \" \".join(current_group)\n            answer = generate_answer_with_gemma(context, question)\n            qa_pairs.append((question, answer, context))\n    \n    return qa_pairs\n\n# Example usage\npdf_path = \"/kaggle/input/attention-is-all-you-need/7181-attention-is-all-you-need.pdf\"\n\n# Step 1: Extract text from PDF\npdf_text = extract_text_from_pdf(pdf_path)\n\n# Step 2: Exclude the references section\npdf_text = exclude_references(pdf_text)\n\n# Step 3: Split the text into sentences\nsentences = split_text_into_sentences(pdf_text)\n\n# Step 4: Generate unique questions and answers based on context relevance\nqa_pairs = generate_unique_questions_and_answers(sentences)\n\n# Step 5: Save generated questions and answers to a file or print them\noutput_file = \"/kaggle/working/generated_questions_and_answers_with_gemma.txt\"\nwith open(output_file, \"w\") as f:\n    for i, (question, answer, context) in enumerate(qa_pairs):\n        f.write(f\"Question {i+1}: {question}\\n\")\n        f.write(f\"Answer {i+1}: {answer}\\n\")\n        f.write(f\"Context {i+1}: {context}\\n\\n\")\n\nprint(f\"{len(qa_pairs)} question-answer pairs generated and saved to '{output_file}'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T13:18:18.933658Z","iopub.execute_input":"2024-12-01T13:18:18.934025Z","iopub.status.idle":"2024-12-01T13:49:06.703575Z","shell.execute_reply.started":"2024-12-01T13:18:18.933993Z","shell.execute_reply":"2024-12-01T13:49:06.701429Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88d3322b8d80446184b25fca86ab324d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f49f5dae6ec145638155ad21301f3a54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"171b31f97ae94d9da093990926b751c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dbe22eb84594c5e866acfd7785bcb31"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.50k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c8630c4be5a440cb14f2655f6ac791f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ee306b3eaeb49218ee6a5ab57836703"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a4faf68777d4aba806ca8b47183b187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ca4d1b224054ddbbdc4a3ea07dcee15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb2d5614dcf64024816c826497921d5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc477e7120924c899bcf119d2b76c941"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21061bcdc0834bc781323f946d7fd353"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b5ce6495b0646e4866570661e436ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef9c90ac000843c88ff7228c531f7792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"407dfec334b94f09aec69c985f45cff3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"798d282d011c4547a651c9bbd6eae950"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77c62f99dfd04fe386b633db519f80e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dae2bc60efeb40c5ae097825dc578294"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1400ea58087f4093a7862d2384f983d4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 210 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec560f4e65614b22945a9222099c98ca"}},"metadata":{}},{"name":"stdout","text":"35 question-answer pairs generated and saved to '/kaggle/working/generated_questions_and_answers_with_gemma.txt'.\n","output_type":"stream"}],"execution_count":13}]}